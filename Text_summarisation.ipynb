{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_summarisation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jacnIZZ4N_t"
      },
      "source": [
        "# -*- coding: utf-8 -*-\r\n",
        "\"\"\"\r\n",
        "Created on Thu Nov 21 16:45:19 2019\r\n",
        "\r\n",
        "@author: Khushwant Rai\r\n",
        "\"\"\"\r\n",
        "from sklearn.manifold import TSNE\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from nltk.tokenize import sent_tokenize\r\n",
        "\r\n",
        "import skipthought\r\n",
        "from sklearn.cluster import KMeans, SpectralClustering\r\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\r\n",
        "from keras.layers import Dense, Input\r\n",
        "from keras.models import Model\r\n",
        "from scipy.sparse.csgraph import laplacian\r\n",
        "import random\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.cm as cm\r\n",
        "from keras.regularizers import l1\r\n",
        "\r\n",
        "class summarizer():\r\n",
        "    tsne = TSNE(n_components=2, random_state=0)\r\n",
        "    def preprocess(self,data):\r\n",
        "        \"\"\"\r\n",
        "        some data prerocessing \r\n",
        "        \"\"\"\r\n",
        "        return data\r\n",
        "            \r\n",
        "            \r\n",
        "    def split_sentences(self,data):\r\n",
        "        sent=[]\r\n",
        "        sentences = sent_tokenize(data)\r\n",
        "        for j in range(len(sentences)):\r\n",
        "            sentences[j] = sentences[j].strip()\r\n",
        "            if sentences[j] != '':\r\n",
        "                sent.append(sentences[j])\r\n",
        "        return sent \r\n",
        "            \r\n",
        "    def skipthought_encode(self,data):\r\n",
        "        all_sentences = [sent for sent in data]\r\n",
        "        print('Loading pre-trained models...')\r\n",
        "        model = skipthought.load_model()\r\n",
        "        encoder = skipthought.Encoder(model)\r\n",
        "        print('Encoding sentences...')\r\n",
        "        enc_sentences = encoder.encode(all_sentences, verbose=0)\r\n",
        "        print(enc_sentences)\r\n",
        "        return enc_sentences\r\n",
        "            \r\n",
        "        \r\n",
        "    def summarize(self,data,x): \r\n",
        "        mean = []\r\n",
        "        closest = []\r\n",
        "        pdata=self.preprocess(data)\r\n",
        "        split=self.split_sentences(pdata)\r\n",
        "        print('tokeniztion done')\r\n",
        "        vectors = self.skipthought_encode(split)\r\n",
        "        print('vetorization done')\r\n",
        "        n_clusters = int(x)\r\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=0)\r\n",
        "        ts_vec=self.tsne.fit_transform(vectors)\r\n",
        "        kmeans = kmeans.fit(ts_vec)\r\n",
        "        for j in range(n_clusters):\r\n",
        "            i = np.where(kmeans.labels_ == j)[0]\r\n",
        "            mean.append(np.mean(i))\r\n",
        "        closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_,ts_vec)\r\n",
        "        ordering = sorted(range(n_clusters), key=lambda k: mean[k])\r\n",
        "        summary = ' '.join([split[closest[kr]] for kr in ordering])\r\n",
        "        print('summary formed')\r\n",
        "        return summary, kmeans, ts_vec, vectors\r\n",
        "    \r\n",
        "    \r\n",
        "    \r\n",
        "    def auto_encoder(self,enc):\r\n",
        "        \r\n",
        "        input_img = Input(shape=(4800,))\r\n",
        "        encoded = Dense(1000, activation='relu', activity_regularizer=l1(10e-6))(input_img)\r\n",
        "        encoded = Dense(200, activation='relu')(encoded)\r\n",
        "        encoded = Dense(10, activation='sigmoid')(encoded)\r\n",
        "        \r\n",
        "        decoded = Dense(10, activation='relu')(encoded)\r\n",
        "        decoded = Dense(200, activation='relu')(decoded)\r\n",
        "        decoded = Dense(1000, activation='relu')(decoded)\r\n",
        "        decoded = Dense(4800)(decoded)\r\n",
        "        autoencoder = Model(input_img, decoded)\r\n",
        "    \r\n",
        "        encoder = Model(input_img, encoded)\r\n",
        "        \r\n",
        "        autoencoder.compile(optimizer='adam', loss='mse')\r\n",
        "        \r\n",
        "        train_x=enc\r\n",
        "        \r\n",
        "        train_x = train_x.reshape(-1, 4800).astype('float32')\r\n",
        "        split_size = int(train_x.shape[0]*0.9)\r\n",
        "        train_x, val_x = train_x[:split_size], train_x[split_size:]\r\n",
        "        \r\n",
        "        autoencoder.fit(train_x, train_x, epochs=500, batch_size=5, validation_data=(val_x, val_x))\r\n",
        "        pred = encoder.predict(train_x)\r\n",
        "        return pred\r\n",
        "        \r\n",
        "    def summarize_autoenc(self,data,vectors,x):\r\n",
        "        mean = []\r\n",
        "        closest = []\r\n",
        "        pdata=self.preprocess(data)\r\n",
        "        split=self.split_sentences(pdata)\r\n",
        "        print('tokeniztion done')\r\n",
        "        #vectors = self.skipthought_encode(split)\r\n",
        "        print('vetorization done')\r\n",
        "        n_clusters = int(x)\r\n",
        "        latent=self.auto_encoder(vectors)\r\n",
        "        kmeans2 = KMeans(n_clusters=n_clusters, random_state=0)\r\n",
        "        ts_vec2=self.tsne.fit_transform(latent)\r\n",
        "        kmeans2 = kmeans2.fit(ts_vec2)\r\n",
        "        for j in range(n_clusters):\r\n",
        "            i = np.where(kmeans2.labels_ == j)[0]\r\n",
        "            mean.append(np.mean(i))\r\n",
        "        closest, _ = pairwise_distances_argmin_min(kmeans2.cluster_centers_,ts_vec2)\r\n",
        "        ordering = sorted(range(n_clusters), key=lambda k: mean[k])\r\n",
        "        summary = ' '.join([split[closest[kr]] for kr in ordering])\r\n",
        "        print('summary formed')\r\n",
        "        return summary, kmeans2, ts_vec2\r\n",
        "    \r\n",
        "    def auto_encoder_spectral(self,lp):  \r\n",
        "        input_img = Input(shape=(len(lp),))\r\n",
        "        \r\n",
        "        # \"encoded\" is the encoded representation of the input\r\n",
        "        encoded = Dense(200, activation='relu')(input_img)\r\n",
        "        encoded = Dense(100, activation='relu')(encoded)\r\n",
        "        encoded = Dense(10, activation='sigmoid')(encoded)\r\n",
        "        \r\n",
        "        # \"decoded\" is the lossy reconstruction of the input\r\n",
        "        decoded = Dense(10, activation='relu')(encoded)\r\n",
        "        decoded = Dense(100, activation='relu')(decoded)\r\n",
        "        decoded = Dense(200, activation='relu')(decoded)\r\n",
        "        decoded = Dense(len(lp))(decoded)\r\n",
        "        autoencoder = Model(input_img, decoded)\r\n",
        "        # this model maps an input to its reconstruction\r\n",
        "        \r\n",
        "        encoder = Model(input_img, encoded)\r\n",
        "        autoencoder.compile(optimizer='adam', loss='mse')\r\n",
        "        autoencoder.fit(lp, lp, epochs=200, batch_size=5)\r\n",
        "        pred = encoder.predict(lp)\r\n",
        "        return pred\r\n",
        "    \r\n",
        "    def summarize_spectral(self,data,vectors,x):\r\n",
        "        mean = []\r\n",
        "        closest = []\r\n",
        "        pdata=self.preprocess(data)\r\n",
        "        split=self.split_sentences(pdata)\r\n",
        "        print('tokeniztion done')\r\n",
        "        #vectors = self.skipthought_encode(split)\r\n",
        "        print('vetorization done')\r\n",
        "        n_clusters = int(x)\r\n",
        "        latent=self.auto_encoder(vectors)\r\n",
        "        spectral = SpectralClustering(affinity='rbf',coef0=1,degree=3,eigen_solver=None,eigen_tol=0.0,gamma=1.0,n_clusters=n_clusters, random_state=0)\r\n",
        "        spectral = spectral.fit(latent)\r\n",
        "        af=spectral.affinity_matrix_\r\n",
        "        lp=laplacian(af,normed=True)\r\n",
        "        lp_latent=self.auto_encoder_spectral(lp)\r\n",
        "        kmeans3 = KMeans(n_clusters=n_clusters, random_state=0)\r\n",
        "        ts_vec3 = self.tsne.fit_transform(lp_latent)\r\n",
        "        kmeans3 = kmeans3.fit(ts_vec3)\r\n",
        "        for j in range(n_clusters):\r\n",
        "            i = np.where(kmeans3.labels_ == j)[0]\r\n",
        "            mean.append(np.mean(i))\r\n",
        "        closest, _ = pairwise_distances_argmin_min(kmeans3.cluster_centers_,ts_vec3)\r\n",
        "        ordering = sorted(range(n_clusters), key=lambda k: mean[k])\r\n",
        "        summary3 = ' '.join([split[closest[kr]] for kr in ordering])\r\n",
        "        print('summary formed')\r\n",
        "        return summary3, kmeans3, ts_vec3\r\n",
        "    \r\n",
        "    def plot(self, ts_vec, kmeans):\r\n",
        "        Y = ts_vec\r\n",
        "        x_coords = Y[:, 0]\r\n",
        "        y_coords = Y[:, 1]\r\n",
        "        # display scatter plot\r\n",
        "        #colors = np.array([x for x in 'bgcmy'])\r\n",
        "        colors=cm.rainbow(np.linspace(0, 1, len(kmeans.labels_)))\r\n",
        "        \r\n",
        "        LABEL_COLOR_MAP = {}\r\n",
        "        for i in range(len(kmeans.labels_)):\r\n",
        "          LABEL_COLOR_MAP[i]=random.choice(colors)\r\n",
        "        \r\n",
        "        label_color = [LABEL_COLOR_MAP[l] for l in kmeans.labels_]\r\n",
        "        \r\n",
        "        plt.scatter(x_coords, y_coords,c=label_color)\r\n",
        "        for label, x, y in zip(kmeans.labels_, x_coords, y_coords):\r\n",
        "            plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\r\n",
        "        centers = kmeans.cluster_centers_\r\n",
        "        \r\n",
        "        x_cent = centers[:, 0]\r\n",
        "        y_cent = centers[:, 1]\r\n",
        "        plt.scatter(x_cent,y_cent, c='black')\r\n",
        "\r\n",
        "\r\n",
        "def split_sentences(data):\r\n",
        "    sent=[]\r\n",
        "    sentences = sent_tokenize(data)\r\n",
        "    for j in range(len(sentences)):\r\n",
        "        sentences[j] = sentences[j].strip()\r\n",
        "        if sentences[j] != '':\r\n",
        "            sent.append(sentences[j])\r\n",
        "    return sent \r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}