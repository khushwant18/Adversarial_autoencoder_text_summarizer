{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_summarisation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jacnIZZ4N_t"
      },
      "source": [
        "# -*- coding: utf-8 -*-\r\n",
        "\"\"\"\r\n",
        "Created on Thu Nov 21 16:45:19 2019\r\n",
        "\r\n",
        "@author: Khushwant Rai\r\n",
        "\"\"\"\r\n",
        "from sklearn.manifold import TSNE\r\n",
        "import numpy as np\r\n",
        "import skipthought\r\n",
        "from sklearn.cluster import KMeans, SpectralClustering\r\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\r\n",
        "from keras.layers import Dense, Input, BatchNormalization\r\n",
        "from keras.layers import PReLU, Subtract, Concatenate\r\n",
        "from keras.models import Model\r\n",
        "import scipy\r\n",
        "from scipy.sparse.csgraph import laplacian\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from keras.regularizers import l1\r\n",
        "from collections import Counter\r\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\r\n",
        "from rouge import Rouge\r\n",
        "from keras.optimizers import Nadam\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "import tensorflow_hub as hub\r\n",
        "import tensorflow as tf\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "import networkx as nx\r\n",
        "import operator\r\n",
        "from tqdm import tqdm\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import re\r\n",
        "\r\n",
        "class isummary():\r\n",
        "    tsne = TSNE(n_components=2, random_state=0, perplexity=30)\r\n",
        "    #get the data from dataset\r\n",
        "    def get_data(self):\r\n",
        "        import pandas as pd\r\n",
        "        df=pd.read_csv('cnn_dataset.csv')\r\n",
        "        x=df.iloc[:,[1]].values\r\n",
        "        y=df.iloc[:,[2]].values\r\n",
        "        return x,y            \r\n",
        "     #tokenize sentences      \r\n",
        "    def split_sentences(self,data):\r\n",
        "        sent=[]\r\n",
        "        sentences = sent_tokenize(data)\r\n",
        "        for j in range(len(sentences)):\r\n",
        "            sentences[j] = sentences[j].strip()\r\n",
        "            if sentences[j] != '':\r\n",
        "                sent.append(sentences[j])\r\n",
        "        return sent \r\n",
        "      #sentence mbedding for clustering based model      \r\n",
        "    def skipthought_encode(self,data):\r\n",
        "        all_sentences = [sent for sent in data]\r\n",
        "        print('Loading pre-trained models...')\r\n",
        "        model = skipthought.load_model()\r\n",
        "        encoder = skipthought.Encoder(model)\r\n",
        "        print('Encoding sentences...')\r\n",
        "        enc_sentences = encoder.encode(all_sentences, verbose=0)\r\n",
        "        print(enc_sentences)\r\n",
        "        return enc_sentences\r\n",
        "            \r\n",
        "    #clustering based refered model\r\n",
        "    def summarize(self,data,x): \r\n",
        "        mean = []\r\n",
        "        closest = []\r\n",
        "        split=self.split_sentences(data)\r\n",
        "        print('tokeniztion done')\r\n",
        "        vectors = self.skipthought_encode(split)\r\n",
        "        print('vetorization done')\r\n",
        "        n_clusters = int(x)\r\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=0)\r\n",
        "        #ts_vec=self.tsne.fit_transform(vectors)\r\n",
        "        ts_vec=vectors\r\n",
        "        kmeans = kmeans.fit(ts_vec)\r\n",
        "        for j in range(n_clusters):\r\n",
        "            i = np.where(kmeans.labels_ == j)[0]\r\n",
        "            mean.append(np.mean(i))\r\n",
        "        closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_,ts_vec)\r\n",
        "        ordering = sorted(range(n_clusters), key=lambda k: mean[k])\r\n",
        "        summary = ' '.join([split[closest[kr]] for kr in ordering])\r\n",
        "        print('summary formed')\r\n",
        "        return summary, kmeans, ts_vec, vectors\r\n",
        "    \r\n",
        "    #autoencoder\r\n",
        "    def auto_encoder(self,enc):\r\n",
        "        input_img = Input(shape=(4800,))\r\n",
        "        encoded = Dense(200, activation='relu', activity_regularizer=l1(10e-6))(input_img)\r\n",
        "        encoded = Dense(50, activation='relu')(encoded)\r\n",
        "        encoded = Dense(10, activation='linear')(encoded)\r\n",
        "        \r\n",
        "        decoded = Dense(10, activation='relu')(encoded)\r\n",
        "        decoded = Dense(50, activation='relu')(decoded)\r\n",
        "        decoded = Dense(200, activation='linear')(decoded)\r\n",
        "        decoded = Dense(4800)(decoded)\r\n",
        "        autoencoder = Model(input_img, decoded)\r\n",
        "    \r\n",
        "        encoder = Model(input_img, encoded)\r\n",
        "        \r\n",
        "        autoencoder.compile(loss=['mse'],\r\n",
        "                                loss_weights=[0.99],\r\n",
        "                                optimizer=Nadam(0.0002, 0.5))\r\n",
        "        \r\n",
        "        train_x=enc\r\n",
        "        split_size = int(train_x.shape[0]*0.9)\r\n",
        "        train_x, val_x = train_x[:split_size], train_x[split_size:]\r\n",
        "        \r\n",
        "        history=autoencoder.fit(train_x, train_x, epochs=200,\r\n",
        "                                batch_size=5, validation_data=(val_x, val_x))\r\n",
        "        pred = encoder.predict(train_x)\r\n",
        "        training_loss = history.history['loss']\r\n",
        "        test_loss = history.history['val_loss']\r\n",
        "        epoch_count = range(1, len(training_loss) + 1)\r\n",
        "        # Visualize \r\n",
        "        plt.plot(epoch_count, training_loss, 'r--')\r\n",
        "        plt.plot(epoch_count, test_loss, 'b-')\r\n",
        "        plt.legend(['Training Loss', 'Test Loss'])\r\n",
        "        plt.xlabel('Epoch')\r\n",
        "        plt.ylabel('Loss')\r\n",
        "        plt.show();\r\n",
        "\r\n",
        "        return pred\r\n",
        "     \r\n",
        "    #clustering based model 1\r\n",
        "    def summarize_autoenc(self,data,vectors,x):\r\n",
        "        mean = []\r\n",
        "        closest = []\r\n",
        "        split=self.split_sentences(data)\r\n",
        "        print('tokeniztion done')\r\n",
        "        #vectors = self.skipthought_encode(split)\r\n",
        "        print('vetorization done')\r\n",
        "        n_clusters = int(x)\r\n",
        "        latent=self.auto_encoder(vectors)\r\n",
        "        kmeans2 = KMeans(n_clusters=n_clusters, random_state=0)\r\n",
        "        ts_vec2=self.tsne.fit_transform(latent)\r\n",
        "        kmeans2 = kmeans2.fit(ts_vec2)\r\n",
        "        for j in range(n_clusters):\r\n",
        "            i = np.where(kmeans2.labels_ == j)[0]\r\n",
        "            mean.append(np.mean(i))\r\n",
        "        closest, _ = pairwise_distances_argmin_min(kmeans2.cluster_centers_,ts_vec2)\r\n",
        "        ordering = sorted(range(n_clusters), key=lambda k: mean[k])\r\n",
        "        summary = ' '.join([split[closest[kr]] for kr in ordering])\r\n",
        "        print('summary formed')\r\n",
        "        return summary, kmeans2, ts_vec2, latent\r\n",
        "    \r\n",
        "    #clustering based model 2\r\n",
        "    def summarize_spectral(self,data,vectors,x, latent):\r\n",
        "        mean = []\r\n",
        "        closest = []\r\n",
        "        split=self.split_sentences(data)\r\n",
        "        print('tokeniztion done')\r\n",
        "\r\n",
        "        print('vetorization done')\r\n",
        "        n_clusters = int(x)\r\n",
        "        spectral = SpectralClustering(affinity='rbf',coef0=1,degree=3,eigen_solver=None,\r\n",
        "                            eigen_tol=0.0,gamma=1.0,n_clusters=n_clusters, random_state=0)\r\n",
        "        spectral = spectral.fit(latent)\r\n",
        "        af=spectral.affinity_matrix_\r\n",
        "        lp=laplacian(af,normed=True)\r\n",
        "        eig_val, eig_vect = scipy.sparse.linalg.eigs(lp, n_clusters)\r\n",
        "        X = eig_vect.real\r\n",
        "        rows_norm = np.linalg.norm(X, axis=1, ord=2)\r\n",
        "        lp_latent = (X.T / rows_norm).T\r\n",
        "\r\n",
        "        kmeans3 = KMeans(n_clusters=n_clusters, random_state=0)\r\n",
        "        ts_vec3 = lp_latent\r\n",
        "        kmeans3 = kmeans3.fit(ts_vec3)\r\n",
        "        for j in range(n_clusters):\r\n",
        "            i = np.where(kmeans3.labels_ == j)[0]\r\n",
        "            mean.append(np.mean(i))\r\n",
        "        closest, _ = pairwise_distances_argmin_min(kmeans3.cluster_centers_,ts_vec3)\r\n",
        "        ordering = sorted(range(n_clusters), key=lambda k: mean[k])\r\n",
        "        summary3 = ' '.join([split[closest[kr]] for kr in ordering])\r\n",
        "        print('summary formed')\r\n",
        "        return summary3, kmeans3, ts_vec3\r\n",
        "\r\n",
        "    #rouge evaluation\r\n",
        "    def evaluate_rouge(self, refs, summ):\r\n",
        "        rouge = Rouge()\r\n",
        "        ref=self.split_sentences(refs)\r\n",
        "        out=self.split_sentences(summ)\r\n",
        "        kscore={}\r\n",
        "        scores = []\r\n",
        "        for i in range(len(out)):  \r\n",
        "          hypothesis = out[i]\r\n",
        "          reference = ref[i]\r\n",
        "          scores.append(rouge.get_scores(hyps=hypothesis,refs=reference)[0])\r\n",
        "\r\n",
        "        sums = dict(Counter(scores[0]['rouge-1']) + Counter(scores[1]['rouge-1']))\r\n",
        "        mean = {k: v / len(ref) for k, v in sums.items()}\r\n",
        "        kscore.update({'rouge-1': mean})\r\n",
        "          \r\n",
        "        return kscore\r\n",
        "\r\n",
        "\r\n",
        "    ########frequecy based model############\r\n",
        "    \r\n",
        "    #prepare frequency table\r\n",
        "    def freq(self,text_string):\r\n",
        "        stop = set(stopwords.words('english'))\r\n",
        "        words = word_tokenize(text_string)\r\n",
        "        porter = PorterStemmer()\r\n",
        "        \r\n",
        "        table = {}\r\n",
        "        \r\n",
        "        for w in words:\r\n",
        "            w = porter.stem(w)\r\n",
        "            if w in stop: \r\n",
        "                continue\r\n",
        "            elif w in table:\r\n",
        "                table[w] += 1\r\n",
        "            else:\r\n",
        "                table[w] = 1           \r\n",
        "        return table\r\n",
        "    \r\n",
        "    #score sentences \r\n",
        "    def score(self, sent, table):\r\n",
        "        sent_val = {}\r\n",
        "        for s in sent:\r\n",
        "            count = len(word_tokenize(s))\r\n",
        "            for val in table:\r\n",
        "                if val.lower() in s.lower():                \r\n",
        "                    if s in sent_val:\r\n",
        "                        sent_val[s] += table[val]\r\n",
        "                    else:\r\n",
        "                        sent_val[s] = table[val]\r\n",
        "            sent_val[s] = sent_val[s] // count\r\n",
        "        return sent_val\r\n",
        "        \r\n",
        "    #summarize\r\n",
        "    def freq_summary(self, sent, sent_val,c):\r\n",
        "        count = 0\r\n",
        "        summary = ''\r\n",
        "        for s in sent:\r\n",
        "            if s in sent_val and count<c:\r\n",
        "                summary += \" \" + s\r\n",
        "                count += 1        \r\n",
        "        return summary\r\n",
        "    \r\n",
        "    ##########graph based model#######\r\n",
        "    \r\n",
        "    #sentence embedding\r\n",
        "    def get_universal_encoder(self,sent):\r\n",
        "  \r\n",
        "        url = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\r\n",
        "        \r\n",
        "        embed = hub.Module(url)\r\n",
        "        \r\n",
        "        tf.logging.set_verbosity(tf.logging.ERROR)\r\n",
        "        with tf.Session() as session:\r\n",
        "            session.run([tf.global_variables_initializer(), tf.tables_initializer()])\r\n",
        "            vectors = session.run(embed(sent))\r\n",
        "        return vectors\r\n",
        "    \r\n",
        "  ###############adversarial autoencoder#################\r\n",
        "  \r\n",
        "      #encoder and decoder setup\r\n",
        "    def encoder(self, latent_dim, cat_dim, input_dim):\r\n",
        "        inputvec = Input(shape=(4800,))\r\n",
        "        encoded = Dense(200, activation='relu')(inputvec)\r\n",
        "        encoded = Dense(50, activation='relu')(encoded)\r\n",
        "        category = Dense(10)(encoded)\r\n",
        "        category = BatchNormalization()(category)\r\n",
        "        category = PReLU()(category)\r\n",
        "        category = Dense(cat_dim, activation='softmax')(category)\r\n",
        "        \r\n",
        "        latent = Dense(50)(encoded)\r\n",
        "        latent = BatchNormalization()(latent)\r\n",
        "        latent = PReLU()(latent)\r\n",
        "        latent = Dense(latent_dim, activation='linear')(latent)\r\n",
        "    \r\n",
        "        decoded = Concatenate()([latent, category])\r\n",
        "        decoded = Dense(50, activation='relu')(decoded)\r\n",
        "        decoded = Dense(200, activation='relu')(decoded)\r\n",
        "        decoded = Dense(4800)(decoded)\r\n",
        "        error = Subtract()([inputvec, decoded]) \r\n",
        "        model = Model(inputvec, [decoded, latent, category, error])\r\n",
        "        return model\r\n",
        "    \r\n",
        "    #generates random vector from normal distribution\r\n",
        "    def random_normal(self, latent_dim, batch_size, window_size=None):\r\n",
        "        shape = (batch_size, latent_dim) \r\n",
        "        return np.random.normal(size=shape)\r\n",
        "    \r\n",
        "    #generate random one hot encoded vectors\r\n",
        "    def random_categories(self, categ, batch_size):\r\n",
        "        cats = np.zeros((batch_size, categ))\r\n",
        "        for i in range(batch_size):\r\n",
        "            one = np.random.randint(0, categ)\r\n",
        "            cats[i][one] = 1\r\n",
        "        return cats\r\n",
        "     \r\n",
        "    #discriminator setup\r\n",
        "    def discriminator(self, latent_dim):\r\n",
        "        input_layer = Input(shape=(latent_dim,))\r\n",
        "        disc = Dense(128)(input_layer)\r\n",
        "        disc = Dense(64)(disc)\r\n",
        "        disc = Dense(1, activation=\"sigmoid\")(disc)\r\n",
        "        model = Model(input_layer, disc)\r\n",
        "        return model\r\n",
        "    \r\n",
        "    #training discriminator\r\n",
        "    def train_disrim(self, discriminator, real, fake):\r\n",
        "        def train(real_samples, fake_samples):\r\n",
        "            discriminator.trainable = True\r\n",
        "    \r\n",
        "            loss_real = discriminator.train_on_batch(real_samples, real)\r\n",
        "            loss_fake = discriminator.train_on_batch(fake_samples, fake)\r\n",
        "            loss = np.add(loss_real, loss_fake) * 0.5\r\n",
        "    \r\n",
        "            discriminator.trainable = False\r\n",
        "    \r\n",
        "            return loss\r\n",
        "        return train\r\n",
        "    \r\n",
        "    #summarizing adversarial encoder generated summary \r\n",
        "    def adversarial_summary(self,pdata, length,vect):\r\n",
        "        split=self.split_sentences(pdata)\r\n",
        "        mean = []\r\n",
        "        closest = []\r\n",
        "        kmeans4 = KMeans(n_clusters=length, random_state=0)\r\n",
        "        kmeans4 = kmeans4.fit(vect)\r\n",
        "        for j in range(length):\r\n",
        "            i = np.where(kmeans4.labels_ == j)[0]\r\n",
        "            mean.append(np.mean(i))\r\n",
        "        closest, _ = pairwise_distances_argmin_min(kmeans4.cluster_centers_,vect)\r\n",
        "        ordering = sorted(range(length), key=lambda k: mean[k])\r\n",
        "        summary4 = ' '.join([split[closest[kr]] for kr in ordering])  \r\n",
        "        return summary4\r\n",
        "\r\n",
        "\r\n",
        "    \r\n",
        "def main():\r\n",
        "    m1_rouge=[]\r\n",
        "    m2_rouge=[]\r\n",
        "    m3_rouge=[]\r\n",
        "    m4_rouge=[]\r\n",
        "    freq_rouge=[]\r\n",
        "    graph_rouge=[]\r\n",
        "    obj=isummary()\r\n",
        "    x,y=obj.get_data()\r\n",
        "\r\n",
        "    for i in range(100):\r\n",
        "        sent = sent_tokenize(x[i][0])\r\n",
        "        length=len(sent_tokenize(y[i][0]))\r\n",
        "        ######freq based######   \r\n",
        "        sent = [re.sub('\\n', '', i) for i in sent]\r\n",
        "        table = obj.freq(\" \".join(sent))\r\n",
        "        scores = obj.score(sent, table)\r\n",
        "        scores = dict(sorted(scores.items(), key=operator.itemgetter(1)))\r\n",
        "        freq_summary = obj.freq_summary(sent, scores,length)\r\n",
        "        freq_rouge.append(obj.evaluate_rouge(y[i][0],freq_summary))\r\n",
        "        \r\n",
        "        ########graph absed#######\r\n",
        "        uni_vector=obj.get_universal_encoder(sent)\r\n",
        "        cos = cosine_similarity(uni_vector)\r\n",
        "        graph = nx.from_numpy_array(cos)\r\n",
        "        graph_scores = nx.pagerank(graph)\r\n",
        "        ranks = sorted(((graph_scores[i],s) for i,s in enumerate(sent)), reverse=True)\r\n",
        "        graph_summary = \" \".join([i[1] for i in ranks[:length]])\r\n",
        "        graph_rouge.append(obj.evaluate_rouge(y[i][0],graph_summary))\r\n",
        "        \r\n",
        "        ########cluster based########\r\n",
        "        summary,tsvec,kmeans,vector=obj.summarize(x[i][0],length)\r\n",
        "        summary2,kmeans2,tsvec2, latent=obj.summarize_autoenc(x[i][0],vector,length)\r\n",
        "        summary3,kmeans3,tsvec3=obj.summarize_spectral(x[i][0],vector,length,latent)\r\n",
        "        m1_rouge.append(obj.evaluate_rouge(y[i][0],summary))\r\n",
        "        m2_rouge.append(obj.evaluate_rouge(y[i][0],summary2))\r\n",
        "        m3_rouge.append(obj.evaluate_rouge(y[i][0],summary3))\r\n",
        "        \r\n",
        "        ############adversarial autoencoder#######\r\n",
        "        train_x=vector\r\n",
        "        input_dim = train_x.shape[1]\r\n",
        "        latent_dim = 10\r\n",
        "        cat_dim = length\r\n",
        "        discrim_norm = obj.discriminator(latent_dim)\r\n",
        "        discrim_norm.compile(loss='binary_crossentropy', \r\n",
        "                                    optimizer='Nadam', \r\n",
        "                                    metrics=['accuracy'])\r\n",
        "    \r\n",
        "        discrim_norm.trainable = False\r\n",
        "    \r\n",
        "        discrim_category = obj.discriminator(cat_dim)\r\n",
        "        discrim_category.compile(loss='binary_crossentropy', \r\n",
        "                                  optimizer='Nadam', \r\n",
        "                                  metrics=['accuracy'])\r\n",
        "    \r\n",
        "        discrim_category.trainable = False\r\n",
        "    \r\n",
        "        encoder = obj.encoder(latent_dim, cat_dim, input_dim)\r\n",
        "    \r\n",
        "        inputvec = Input(shape=(4800,))\r\n",
        "        reconstructed_signal, encoded_repr, category, _ = encoder(inputvec)\r\n",
        "    \r\n",
        "        norm = discrim_norm(encoded_repr)\r\n",
        "        categ = discrim_category(category)\r\n",
        "    \r\n",
        "        autoencoder = Model(inputvec, [reconstructed_signal, norm, categ])\r\n",
        "        autoencoder.compile(loss=['mse', 'binary_crossentropy', 'binary_crossentropy'],\r\n",
        "                                        optimizer='Nadam')\r\n",
        "    \r\n",
        "        batches = 2000\r\n",
        "        batch_size=64\r\n",
        "    \r\n",
        "        lenc = []\r\n",
        "        lval = []\r\n",
        "        rl = np.ones((batch_size, 1))\r\n",
        "        fk = np.zeros((batch_size, 1))\r\n",
        "    \r\n",
        "        train_discrim_norm = obj.train_disrim(discrim_norm, rl, fk)\r\n",
        "        train_dicrim_categ = obj.train_disrim(discrim_category, rl, fk)\r\n",
        "    \r\n",
        "        start = tqdm(range(batches))\r\n",
        "    \r\n",
        "        for _ in start:\r\n",
        "          \r\n",
        "            ids = np.random.randint(0, train_x.shape[0], batch_size)\r\n",
        "            vect = train_x[ids]\r\n",
        "    \r\n",
        "            _, flatent, fcategory, _ = encoder.predict(vect)\r\n",
        "    \r\n",
        "            rlatent = obj.random_normal(latent_dim, batch_size)\r\n",
        "            rcategory = obj.random_categories(cat_dim, batch_size)\r\n",
        "    \r\n",
        "            norm_loss = train_discrim_norm(rlatent, flatent)\r\n",
        "            categ_loss = train_dicrim_categ(rcategory, fcategory)\r\n",
        "            \r\n",
        "            enc_loss = autoencoder.train_on_batch(vect, [vect, rl, rl])\r\n",
        "            lenc.append(enc_loss)\r\n",
        "    \r\n",
        "            val_loss = autoencoder.test_on_batch(vect, [vect, rl, rl])\r\n",
        "            lval.append(val_loss)\r\n",
        "    \r\n",
        "            start.set_description(\"[normal_disc_acc:  %.2f%% Category_disc_acc: %.2f%%] [MSE train: %f val: %f]\"\r\n",
        "                                 %(100*norm_loss[1], 100*categ_loss[1], enc_loss[1], val_loss[1]))\r\n",
        "            \r\n",
        "        (dec, rep, cat, error) = encoder.predict(train_x)\r\n",
        "        summary4=obj.adversarial_summary(x[i][0],length,rep)\r\n",
        "        m4_rouge.append(obj.evaluate_rouge(y[i][0],summary4))\r\n",
        "        plt.plot([loss[1] for loss in lenc])\r\n",
        "        plt.plot([loss[1] for loss in lval])\r\n",
        "        plt.legend(['Training Loss', 'Test Loss'])\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}